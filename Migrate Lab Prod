ğŸ”¹ 1ï¸âƒ£ Kernel & Networking Validation (ALL NODES)

ğŸ“ RUN ON: CONTROL + WORKERS

lsmod | grep br_netfilter || modprobe br_netfilter

sysctl net.bridge.bridge-nf-call-iptables
sysctl net.bridge.bridge-nf-call-ip6tables
sysctl net.ipv4.ip_forward


If NOT set, persist:

cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF

sysctl --system


âœ” Required for stable networking
âŒ No reboot needed

ğŸ”¹ 2ï¸âƒ£ DNS: LAB TEMP FIX âœ PROD SAFE FIX

ğŸ“ RUN ON: ALL NODES

âŒ REMOVE any /etc/resolv.conf overwrite

chattr -i /etc/resolv.conf 2>/dev/null || true


âœ” Configure DNS via NetworkManager:

nmcli con show
nmcli con mod <connection-name> ipv4.ignore-auto-dns yes
nmcli con mod <connection-name> ipv4.dns "8.8.8.8 1.1.1.1"
nmcli con up <connection-name>


âœ” CoreDNS will stabilize after this

ğŸ”¹ 3ï¸âƒ£ Firewall: LAB OPEN âœ PROD CONTROLLED

ğŸ“ RUN ON: ALL NODES

systemctl enable firewalld
systemctl start firewalld


ğŸ“ CONTROL-PLANE

firewall-cmd --permanent --add-port=6443/tcp
firewall-cmd --permanent --add-port=10250/tcp
firewall-cmd --permanent --add-port=30000-32767/tcp
firewall-cmd --reload


ğŸ“ WORKERS

firewall-cmd --permanent --add-port=10250/tcp
firewall-cmd --permanent --add-port=30000-32767/tcp
firewall-cmd --reload


âœ” Safe NodePort exposure
âœ” kubelet secure

ğŸ”¹ 4ï¸âƒ£ SELinux: Disabled âœ Permissive

ğŸ“ RUN ON: ALL NODES

sed -i 's/^SELINUX=.*/SELINUX=permissive/' /etc/selinux/config
setenforce 0
getenforce


âœ” Production-compliant
âŒ Never leave disabled

ğŸ”¹ 5ï¸âƒ£ CNI & CoreDNS Stabilization

ğŸ“ RUN ON: CONTROL-PLANE ONLY

Restart CoreDNS (post-network fix):

kubectl delete pod -n kube-system -l k8s-app=kube-dns


Validate:

kubectl get pods -n kube-system -l k8s-app=kube-dns


Test:

kubectl run -it --rm busybox \
--image=busybox --restart=Never -- \
nslookup kubernetes.default.svc.cluster.local


âœ” This confirms LAB DNS hacks are gone

ğŸ”¹ 6ï¸âƒ£ Storage Migration: Local-Path âœ Longhorn

ğŸ“ CONTROL-PLANE ONLY

Aï¸âƒ£ Install Longhorn (if not already)
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml
kubectl get pods -n longhorn-system -w

Bï¸âƒ£ Change Default StorageClass
kubectl patch storageclass longhorn \
-p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

kubectl patch storageclass local-path \
-p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'


âœ” New PVCs will use Longhorn

ğŸ”¹ 7ï¸âƒ£ AWX Data Migration (NO DATA LOSS)

ğŸ“ CONTROL-PLANE ONLY

1ï¸âƒ£ Scale AWX down:

kubectl scale deployment awx-operator -n awx --replicas=0
kubectl scale deployment awx-demo-web -n awx --replicas=0
kubectl scale deployment awx-demo-task -n awx --replicas=0


2ï¸âƒ£ Patch AWX to use Longhorn:

kubectl patch awx awx-demo -n awx --type merge -p '
spec:
  postgres_storage_class: longhorn
  projects_storage_class: longhorn
'


3ï¸âƒ£ Scale back up:

kubectl scale deployment awx-operator -n awx --replicas=1


âœ” PostgreSQL & project data preserved

ğŸ”¹ 8ï¸âƒ£ Upgrade AWX Operator (LAB âœ PROD)

ğŸ“ CONTROL-PLANE ONLY

kubectl delete deployment awx-operator -n awx
kubectl apply -f https://github.com/ansible/awx-operator/releases/download/2.7.2/awx-operator.yaml
kubectl get pods -n awx


âœ” Locks operator version
âœ” Avoids devel branch risk

ğŸ”¹ 9ï¸âƒ£ Node Hardening Checks

ğŸ“ CONTROL-PLANE ONLY

kubectl get nodes
kubectl describe node | egrep "Ready|NetworkUnavailable"


ğŸ“ ALL NODES

df -h
free -m
uptime

ğŸ”¹ ğŸ”Ÿ Production Acceptance Tests

âœ” DNS
âœ” Pod-to-Pod
âœ” Storage
âœ” AWX UI

kubectl get pods -A
kubectl get pvc -A
kubectl get svc -n awx


Access:

http://<NODE-IP>:<NodePort>
