This is COMPLETE and PRODUCTION-SAFE based on your final working state.

ğŸ”¹ 0ï¸âƒ£ Kernel + Networking Fix

ğŸ“ RUN ON: ALL NODES (Control-plane + Workers)

modprobe br_netfilter
echo br_netfilter > /etc/modules-load.d/br_netfilter.conf

sysctl -w net.bridge.bridge-nf-call-iptables=1
sysctl -w net.bridge.bridge-nf-call-ip6tables=1
sysctl -w net.ipv4.ip_forward=1

cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF

sysctl --system

ğŸ”¹ 0ï¸âƒ£A DNS TEMP FIX

ğŸ“ RUN ON: ALL NODES

cat <<EOF > /etc/resolv.conf
search localdomain vgs.com
nameserver 8.8.8.8
nameserver 1.1.1.1
EOF

ğŸ”¹ 1ï¸âƒ£ Disable Firewall & SELinux

ğŸ“ RUN ON: ALL NODES

systemctl stop firewalld
systemctl disable firewalld
iptables -F
setenforce 0
getenforce

ğŸ”¹ 2ï¸âƒ£ Clean Old CNI State

ğŸ“ RUN ON: CONTROL-PLANE ONLY

systemctl stop kubelet

ip link delete cni0 || true
ip link delete flannel.1 || true

rm -rf /var/lib/cni/networks/*
rm -rf /var/lib/cni/bin/*
rm -rf /etc/cni/net.d/*

systemctl start kubelet

ğŸ”¹ 3ï¸âƒ£ Initialize Kubernetes

ğŸ“ RUN ON: CONTROL-PLANE ONLY

kubeadm init --pod-network-cidr=10.244.0.0/16 --upload-certs

export KUBECONFIG=/etc/kubernetes/admin.conf

kubectl taint nodes --all node-role.kubernetes.io/control-plane-

ğŸ”¹ 4ï¸âƒ£ Join Worker Nodes

ğŸ“ RUN ON: CONTROL-PLANE (GET COMMAND)

kubeadm token create --print-join-command


ğŸ“ RUN ON: EACH WORKER NODE (PASTE OUTPUT)

kubeadm join <CONTROL-PLANE-IP>:6443 --token <TOKEN> \
--discovery-token-ca-cert-hash sha256:<HASH>

ğŸ”¹ 5ï¸âƒ£ Install Flannel CNI

ğŸ“ RUN ON: CONTROL-PLANE ONLY

kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

kubectl get pods -n kube-flannel -o wide

kubectl get nodes -o wide

ğŸ”¹ 6ï¸âƒ£ Restart CoreDNS

ğŸ“ RUN ON: CONTROL-PLANE ONLY

kubectl delete pod -n kube-system -l k8s-app=kube-dns

kubectl get pods -n kube-system -l k8s-app=kube-dns

ğŸ”¹ 7ï¸âƒ£ Install Local-Path Storage

ğŸ“ RUN ON: CONTROL-PLANE ONLY

kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml

kubectl patch storageclass local-path \
-p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

kubectl get storageclass

ğŸ”¹ 8ï¸âƒ£ Install Longhorn

ğŸ“ RUN ON: CONTROL-PLANE ONLY

kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml

kubectl get pods -n longhorn-system -w

kubectl port-forward svc/longhorn-frontend -n longhorn-system 8080:80

ğŸ”¹ 9ï¸âƒ£ Deploy AWX Operator (PRODUCTION VERSION)

ğŸ“ RUN ON: CONTROL-PLANE ONLY

kubectl create namespace awx

kubectl apply -f https://github.com/ansible/awx-operator/releases/download/2.7.2/awx-operator.yaml

kubectl get pods -n awx

ğŸ”¹ ğŸ”Ÿ Create AWX Admin Password

ğŸ“ RUN ON: CONTROL-PLANE ONLY

kubectl create secret generic awx-admin-password \
--from-literal=password="ProdSecretPassword123" \
-n awx

ğŸ”¹ 1ï¸âƒ£1ï¸âƒ£ Deploy AWX Instance

ğŸ“ RUN ON: CONTROL-PLANE ONLY

cat <<EOF | kubectl apply -f -
apiVersion: awx.ansible.com/v1beta1
kind: AWX
metadata:
  name: awx-demo
  namespace: awx
spec:
  service_type: NodePort
EOF

kubectl get pods -n awx -o wide

kubectl get svc -n awx

ğŸ”¹ 1ï¸âƒ£2ï¸âƒ£ Connectivity Tests

ğŸ“ RUN ON: CONTROL-PLANE ONLY

kubectl run -it --rm busybox \
--image=busybox --restart=Never -- \
nslookup kubernetes.default.svc.cluster.local

kubectl run -it --rm busybox \
--image=busybox --restart=Never -- \
ping -c 3 10.96.0.1

ğŸ”¹ 1ï¸âƒ£3ï¸âƒ£ Access AWX

ğŸ“ RUN ON: CONTROL-PLANE ONLY

kubectl get svc -n awx

http://<ANY-NODE-IP>:<NodePort>

